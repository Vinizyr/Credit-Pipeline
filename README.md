# Credit-Pipeline

ğŸš€ Construindo um Projeto Real de Engenharia de Dados (Foco em CrÃ©dito)

Nas Ãºltimas semanas venho estruturando um projeto de Engenharia de Dados com foco em me preparar para atuar como Engenheiro de Dados Pleno na Ã¡rea de crÃ©dito.

O objetivo nÃ£o era apenas â€œrodar PySparkâ€, mas construir algo com arquitetura prÃ³xima de ambiente produtivo.

ğŸ“Š Dataset

Utilizei o Brazilian E-Commerce Public Dataset by Olist como base transacional.

ğŸ— Arquitetura Implementada

Estruturei o projeto como um Data Lake com camadas:

Landing â†’ Bronze â†’ Silver â†’ Gold

ğŸ¥‰ Bronze

ConversÃ£o de CSV para Parquet

Schema explÃ­cito (sem inferSchema)

Colunas tÃ©cnicas (ingestion_timestamp, source)

Estrutura pronta para auditoria e reprocessamento

ğŸ¥ˆ Silver

Tipagem correta

ConversÃ£o de datas

NormalizaÃ§Ã£o de strings

DeduplicaÃ§Ã£o

Dados confiÃ¡veis para consumo analÃ­tico

ğŸ¥‡ Gold

CriaÃ§Ã£o de modelo dimensional (Star Schema):

dim_customers

dim_date

fact_orders

Uso de surrogate keys e separaÃ§Ã£o clara entre fatos e dimensÃµes.

ğŸ“ˆ EvoluÃ§Ã£o: Camada AnalÃ­tica com MÃ©tricas

AlÃ©m do modelo dimensional, evoluÃ­ o projeto para gerar uma tabela analÃ­tica com indicadores financeiros, como:

Taxa de cancelamento

Volume de pedidos por estado

Tempo mÃ©dio entre compra e aprovaÃ§Ã£o

FrequÃªncia de clientes recorrentes

Indicadores de comportamento transacional

Essa camada jÃ¡ permite alimentar dashboards ou anÃ¡lises estratÃ©gicas.

ğŸ§  Boas prÃ¡ticas aplicadas

âœ” Estrutura modular (jobs/, schemas/, utils/)
âœ” SeparaÃ§Ã£o clara de responsabilidades
âœ” Pipeline idempotente
âœ” Controle de schema
âœ” OrganizaÃ§Ã£o pronta para orquestraÃ§Ã£o
âœ” Preparado para evoluÃ§Ã£o para ingestÃ£o incremental e Delta Lake

âš™ Desafios resolvidos

ConfiguraÃ§Ã£o do Spark no Windows (winutils.exe)

EstruturaÃ§Ã£o de projeto Python como pacote

OrganizaÃ§Ã£o de imports e modularizaÃ§Ã£o

Pensar performance (shuffle, partiÃ§Ãµes, parquet vs csv)

ğŸ¯ Principal aprendizado

Engenharia de Dados nÃ£o Ã© sÃ³ transformar dados.

Ã‰ pensar em:

Escalabilidade

GovernanÃ§a

Reprocessamento

Performance

Modelo analÃ­tico

Valor de negÃ³cio
